<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>扩散模型</title>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markdown-it-texmath@1.0.0/css/texmath.min.css">
    
    <style>
        body {
            font-family: -apple-system, "Segoe UI", sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
            background-color: #f4f4f4;
        }
        #content {
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 12px rgba(0,0,0,0.1);
        }
        .katex-display { overflow-x: auto; padding: 10px 0; }
    </style>
</head>
<body>

    <div id="content">正在加载渲染...</div>

    <script src="https://cdn.jsdelivr.net/npm/markdown-it@13.0.1/dist/markdown-it.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/markdown-it-texmath@1.0.0/texmath.min.js"></script>

    <script type="text/plain" id="markdown-source">

        ## 扩散模型

扩散模型（Diffusion Model）是目前人工智能领域，尤其是在图像生成、视频合成等生成式AI（Generative AI）中最为核心的技术方案。它是 Midjourney、Stable Diffusion 和 DALL-E 等顶尖绘图工具背后的底层逻辑。

简单来说，扩散模型的过程就像是**“将一滴墨水在水中化开（破坏），然后再让时间倒流，让墨水重新聚集成原来的形状（重建）”**。

------



## 1. 核心工作原理

扩散模型的工作流程主要分为两个完全相反的过程：**前向扩散**和**反向去噪**。

### A. 前向扩散过程 (Forward Diffusion)

这是一个逐渐给数据添加噪声的过程。

- **过程：** 假设我们有一张清晰的图像（$x_0$），我们分多步（比如 1000 步）往里面添加微小的随机高斯噪声。
- **结果：** 随着步骤增加，原图的结构逐渐模糊，直到最后变成一张完全没有任何信息的纯随机噪声图（$x_T$）。
- **特点：** 这个过程是确定的，不需要学习，就像把照片放进碎纸机里。

### B. 反向去噪过程 (Reverse Diffusion)

这是模型真正展现“智能”的地方。

- **核心任务：** 训练一个神经网络（通常是 **UNet**），让它学习如何从充满噪声的状态中一步步还原出清晰的图像。
- **原理：** 给定第 $t$ 步的噪声图，模型尝试预测在前向过程中这一步添加了多少噪声。
- **生成：** 在推理阶段，我们从一段纯随机噪声开始，让模型不断地“擦除”它预测出的噪声。经过成百上千次的迭代，图像从杂乱无章的像素中逐渐显现出来。

------

## 2. 关键组件：UNet 与 采样器

为了让上述过程跑通，扩散模型依赖几个关键架构：

### UNet 神经网络

UNet 是扩散模型的“大脑”。它的结构特点是：

1. **压缩阶段：** 提取图像的高维特征。
2. **扩张阶段：** 将特征还原回原始尺寸。
3. **跳跃连接（Skip Connections）：** 确保在去噪过程中不会丢失图像的细节边缘。

### 调度器/采样器 (Scheduler/Sampler)

去噪不是一步完成的，而是分步迭代。采样器决定了：

- 每一步去噪的“步长”是多少。
- 使用什么样的数学公式来计算下一步的像素值（例如 DDIM, Euler a 等）。

------

## 3. 为什么扩散模型优于以往的模型？

在扩散模型流行之前，生成领域主要是 GAN（生成对抗网络）和 VAE（变分自编码器）的天下。

| **模型类型**  | **优点**                                     | **缺点**                                                   |
| ------------- | -------------------------------------------- | ---------------------------------------------------------- |
| **GAN**       | 生成速度极快，图像锐利。                     | 训练极不稳定，容易出现“模式崩溃”（翻来复去生成那几张图）。 |
| **VAE**       | 训练稳定，数学原理优美。                     | 生成的图像往往比较模糊。                                   |
| **Diffusion** | **生成质量极高，多样性极强，训练非常稳定。** | 生成速度较慢（需要多次迭代采样）。                         |

------

## 4. 潜空间扩散 (Latent Diffusion)

早期的扩散模型直接在像素级别操作，计算量巨大。2022 年由 Stability AI 推动的 **Stable Diffusion** 引入了“潜空间（Latent Space）”的概念：

- 它先用一个编码器把高清图压缩成一个很小的矩阵。

- **扩散过程只在这些压缩后的数据上进行。**

- 最后再用解码器还原回大图。

  这大大降低了对显卡的硬件需求，让普通用户在家里也能运行生成模型。

------

## 5. 当前的演进：Diffusion Transformer (DiT)

2024 年以来（例如 OpenAI 的 **Sora** 以及最新的 Stable Diffusion 3），扩散模型开始弃用传统的 UNet，转而使用 **Transformer** 架构。

- **优势：** Transformer 具有更强的扩展性（Scaling Law）。这意味着只要投入更多的算力和数据，模型的理解能力和生成质量就会呈指数级增长。

> 总结：
>
> 扩散模型通过“学习如何去噪”这一物理学启发的逻辑，掌握了从混沌中创造秩序的能力。它不仅能画画，现在正在向视频生成、音频合成、甚至蛋白质结构预测等领域快速扩张。

------

**你想深入了解其中的某一部分吗？** 比如：

1. 如何编写 **Prompt（提示词）** 来更好地控制生成结果？
2. 扩散模型背后的**数学推导**（高斯分布与贝叶斯定理）？
3. 如何在本地电脑上**部署**一个 Stable Diffusion？

## Diffusion Transformer (DiT)

**Diffusion Transformer (DiT)** 是当前生成式 AI 领域最前沿的架构之一。它在 2023 年由 William Peebles 和 Saining Xie 在论文《Scalable Diffusion Models with Transformers》中首次提出。

如果说传统的扩散模型（如 Stable Diffusion 1.5/2.1）使用的是 **UNet** 作为大脑，那么 DiT 就是将这个大脑换成了 **Transformer**。这一改动直接引发了生成模型（尤其是视频生成，如 OpenAI 的 Sora）的爆发式增长。

------

## 1. 核心思想：为什么要换掉 UNet？

在 DiT 出现之前，扩散模型几乎等同于 **卷积神经网络 (CNN)** 架构的 UNet。

- **UNet 的局限：** 虽然卷积在图像处理上很强，但它带有强烈的“归纳偏置”（Inductive Bias），即倾向于处理局部像素。当模型参数量极其庞大时，这种偏置反而限制了模型的扩展性（Scalability）。
- **Transformer 的优势：** Transformer 结构简单、通用，且极其擅长**扩展（Scaling）**。在自然语言处理（GPT）领域，Transformer 已经证明了“只要模型够大、数据够多，能力就会产生质变”。DiT 的目标就是将这种“缩放定律（Scaling Law）”引入图像/视频生成。

------

## 2. DiT 的架构详解

DiT 的工作流程可以分为三个阶段：

### A. 潜空间切片 (Patchify)

DiT 并不直接在原始像素上操作，而是在 **Latent Space（潜空间）** 运行。

1. **输入：** 经过 VAE 压缩后的 $32 \times 32 \times 4$ 的潜变量。
2. **切片：** 将其切成一个个小的“Patch”（例如 $2 \times 2$）。
3. **线性映射：** 将每个 Patch 展平并映射成一系列 **Token（向量序列）**，这与 GPT 处理单词的方式如出一辙。

### B. 核心 DiT Block（调节机制）

DiT 最具创新性的是它如何在 Transformer 中引入控制信息（如时间步 $t$、分类标签或文本描述）。论文研究了四种变体，其中 **adaLN-Zero** 效果最好：

- **In-context conditioning：** 简单将控制向量作为额外的 Token 加入序列。
- **Cross-attention：** 像 Stable Diffusion 那样使用交叉注意力（计算量较大）。
- **adaLN (Adaptive Layer Norm)：** 通过预测缩放和平移参数来调节特征。
- **adaLN-Zero (关键创新)：** 它不仅调节缩放，还学习一个“门控（Gate）”系数，在初始化时将每个 Block 设为恒等映射（Identity Function）。这让训练变得极度稳定。

### C. 还原与解码 (Depatchify)

经过多层 Transformer 交互后，模型将输出的 Token 重新排列回图像格点形状，再由 VAE 解码器还原成高清图像。

------

## 3. DiT 的三大优势

1. **极强的扩展性（Scalability）：** DiT 的性能（以 FID 分数衡量）与计算量（Gflops）之间存在极强的正相关。你可以通过**增加层数、增加宽度**或**减小 Patch 大小**来稳步提升画质。
2. **效率更高：** 相比于复杂的 UNet 结构，Transformer 的计算更具规律性，非常适合在现代 GPU 上进行大规模并行优化。
3. **统一化架构：** 由于文本（GPT）、图像（ViT/DiT）和视频现在都统一到了 Transformer 架构下，这为实现真正的“原生多模态”铺平了道路。

------

## 4. 行业应用：谁在用 DiT？

目前最顶尖的生成模型几乎全部转向了 DiT 架构：

| **模型**               | **开发者**        | **特点**                                                     |
| ---------------------- | ----------------- | ------------------------------------------------------------ |
| **Sora**               | OpenAI            | 视频生成的里程碑，本质上是“视频版”的 DiT。                   |
| **Stable Diffusion 3** | Stability AI      | 引入了 **MM-DiT**（多模态 DiT），让文本和图像特征在同一层进行交互。 |
| **FLUX.1**             | Black Forest Labs | 2024 年爆火的开源模型，使用 **12B 参数** 的 Rectified Flow Transformer。 |
| **Grok-3**             | xAI               | 最新的生成能力同样基于大规模 DiT 架构。                      |

------

### 总结

Diffusion Transformer 的意义在于：它证明了**扩散模型并不依赖于特定的卷积结构**。通过将 Transformer 的强大扩展能力赋予扩散模型，AI 终于能够像理解文字一样，深度理解并生成复杂、高连贯性的视觉内容。

**您想更深入了解其中的哪一部分？**

1. 这种架构在 **Sora 视频生成** 中是如何处理时间维度的？
2. **adaLN-Zero** 具体的数学计算公式？
3. 如何在代码层面实现一个简单的 DiT 模块？

Diffusion Transformer Explained: The Architecture Behind Sora and SD3

这个视频详细拆解了 DiT 的内部组件（如 Patchify 和 adaLN-Zero），非常适合通过视觉化展示来帮助你理解其工作流程。

## Stable Diffusion 3 (SD3)

**Stable Diffusion 3 (SD3)** 是 Stability AI 在 2024 年发布的最先进的开源文生图模型系列。它是对早期版本（如 SD1.5 和 SDXL）的一次重大技术飞跃，其核心目标是解决以往模型在**文字渲染**、**复杂提示词遵循**以及**图像质量**上的瓶颈。

SD3 的架构不再沿用传统的 UNet，而是全面转向了 **Transformer**。

------

## 1. 核心技术架构：MM-DiT

SD3 最核心的创新在于其 **多模态扩散 Transformer (Multimodal Diffusion Transformer, MM-DiT)** 架构。

- **双流并行处理：** 与以往将文本特征“注入”到图像网络的方式不同，MM-DiT 为**文本**和**图像**两套模态分别准备了两组独立的权重（Weights）。
- **联合注意力机制（Joint Attention）：** 虽然两者的权重是分开的，但在处理过程中，图像的 Token 和文本的 Token 会合并在一起进行注意力计算。这意味着图像特征会根据文本来调整，同时文本特征也会根据正在生成的图像内容进行优化。
- **优势：** 这种“双向奔赴”的机制极大地提升了模型对复杂指令的理解能力，尤其是在处理多个物体的位置关系、动作交互时表现非常出色。

------

## 2. 数学引擎：修正流 (Rectified Flow)

SD3 引入了一种新的数学方案来替代传统的扩散公式，称为 **Rectified Flow (RF)**。

- **直线化路径：** 传统的扩散模型中，从噪声到图像的还原路径是弯曲且复杂的。修正流通过学习让这个路径尽可能变成一条**直线**。
- **效率提升：** 因为路径是直的，模型在生成时只需要极少的步数（甚至 4-10 步）就能达到很高的画质。
- **重加权技术：** SD3 在训练中给路径的“中间阶段”分配了更多权重。因为实验证明，中间阶段是决定图像大轮廓和核心特征的关键，这一改进显著提升了生成的稳定性。

------

## 3. 强大的文本理解：T5 编码器

SD3 在文本处理上使用了“三保险”方案，集成了三个不同的文本编码器：

1. **CLIP-L** 和 **CLIP-G：** 用于捕捉图像的整体风格和视觉美感。
2. **T5-XXL：** 这是一个拥有数十亿参数的大语言模型编码器。

**T5 的加入是 SD3 能够准确“写字”的关键。** 它让模型具备了强大的语义理解能力，能够理解长难句、逻辑关系，并能准确在图中拼写出用户指定的单词。

------

## 4. 模型版本与演进 (SD3.5)

为了适配不同的硬件环境，SD3 采用了参数化方案，范围从 **8 亿 (800M)** 到 **80 亿 (8B)** 参数不等。

- **SD3 Medium (2B)：** 适合普通家用显卡，平衡了性能与体积。
- **SD3 Large (8B)：** 旗舰模型，细节和语义遵循能力最强。
- **SD3.5 系列：** 在 2024 年底，针对 3.0 版本在人体结构和某些风格上的争议，官方推出了 3.5 版本（包括 Large、Large Turbo 和 Medium），进一步优化了图像生成的自然度和多样性。

------

## 5. SD3 vs SDXL：有哪些进步？

| **特性**     | **SDXL**                 | **Stable Diffusion 3**                    |
| ------------ | ------------------------ | ----------------------------------------- |
| **底层架构** | 基于 CNN 的 UNet         | 基于 Transformer 的 MM-DiT                |
| **文字渲染** | 较弱，经常拼错或变成乱码 | **极强**，可准确渲染复杂招牌、海报文字    |
| **指令遵循** | 能理解简单物体组合       | **极强**，能处理复杂位置、颜色和动作细节  |
| **硬件要求** | 8G-12G 显存即可流畅运行  | 2B 版本需求类似，8B 版本需要 16G 以上显存 |

------

### 总结

Stable Diffusion 3 的出现标志着开源图像生成正式进入了 **“Transformer + 修正流”** 的时代。它不再只是一个“画图工具”，而是一个能够深度理解人类语言逻辑、并能精准控制画面每一个元素的视觉大脑。

**你想进一步了解如何使用 SD3 吗？**

1. 如何在 ComfyUI 中**搭建** SD3 的工作流？
2. 如何通过 **Prompt 技巧** 充分发挥 SD3 的文字渲染能力？
3. 想了解 SD3.5 在**人体结构优化**上的具体改进？

Stable Diffusion 3: MASSIVE Improvements, Better than SDXL and SORA?

这段视频详细对比了 Stable Diffusion 3 与前代模型的性能差异，并直观展示了它在文字生成和复杂指令遵循方面的巨大进步。

    </script>

    <script>
        window.onload = function() {
            try {
                // 初始化渲染器
                const md = window.markdownit({
                    html: true,
                    breaks: true,
                    linkify: true
                }).use(texmath, {
                    engine: window.katex,
                    delimiters: 'dollars',
                    katexOptions: { macros: { "\\RR": "\\mathbb{R}" } }
                });

                // 【核心修复】使用 textContent 替代 innerHTML
                // 这能保证拿到原始的 & 和 < 符号
                const rawSource = document.getElementById('markdown-source').textContent;
                
                // 执行渲染
                const htmlOutput = md.render(rawSource);
                
                // 写入页面
                document.getElementById('content').innerHTML = htmlOutput;
            } catch (err) {
                console.error("渲染错误:", err);
                document.getElementById('content').innerHTML = "渲染失败，请查看控制台错误信息。";
            }
        };
    </script>
</body>
</html>